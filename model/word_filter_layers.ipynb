{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraForPreTraining, ElectraTokenizerFast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ElectraForPreTraining.from_pretrained('Seznam/small-e-czech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ElectraTokenizerFast.from_pretrained(\"Seznam/small-e-czech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_filter import WordFilterLayers, FilterBuilderWithTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = WordFilterLayers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_builder = FilterBuilderWithTokenizer('stenozaznamy_psp_sample.pickle', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulles = filter_builder.get_nulls()\n",
    "# for nul in sorted(nulles, key=(lambda x: nulles[x]), reverse=True):\n",
    "#     print(f'{nul}:{nulles[nul]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = WordFilterLayers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()\n",
    "optimizer = optim.Adam(layer.parameters(), lr=1e-4)\n",
    "\n",
    "labels = {word: 1.0 * (nulles[word] > 0.9) for word in nulles}\n",
    "counter = 0\n",
    "for paragraph in filter_builder.paragraphs():\n",
    "    counter += 1\n",
    "    labels_t = []\n",
    "    # for word in filter_builder.word_buffer(paragraph):\n",
    "    #     encodings = tokenizer.encode(word)\n",
    "    #     for encoding in encodings[1:-1]:\n",
    "    #         if word in labels:\n",
    "    #             labels_t.append((labels[word], encoding))\n",
    "    #         else:\n",
    "    #             labels_t.append((1.0, encoding))\n",
    "    # print(tokenizer.encode(paragraph))\n",
    "    # print(paragraph_encodings)\n",
    "    #   paragraph_encodings = torch.tensor(paragraph_encodings)\n",
    "    # counter = 0\n",
    "    # for i in range(len(labels_t)):\n",
    "    #     label_t = labels_t[counter] \n",
    "    #     while label_t[1] != paragraph_encodings[0,counter]:\n",
    "    #         labels_t.insert(i, (0.0, paragraph_encodings[0, counter]))\n",
    "    #         counter += 1\n",
    "    #     else:\n",
    "    #         counter += 1\n",
    "    # print(pred.last_hidden_state)\n",
    "    # labels_t.append((0.0, 1))\n",
    "    # labels_t.insert(0, (0.0,)\n",
    "    paragraph_encodings = tokenizer.encode(paragraph, return_tensors='pt')\n",
    "    if len(paragraph_encodings[0]) > 512:\n",
    "        paragraph_encodings = paragraph_encodings[:, 0:512]\n",
    "    for encoding in paragraph_encodings[0]:\n",
    "        if encoding.item() not in labels:\n",
    "            labels_t.append(0.0)\n",
    "        else:\n",
    "            labels_t.append(labels[encoding.item()])\n",
    "    pred = model.electra(paragraph_encodings)\n",
    "    res = layer(pred.last_hidden_state)\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_value = loss(res.squeeze(0).squeeze(1), torch.tensor(labels_t))\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    if counter % 10 == 0:\n",
    "        print(f'{loss_value.item()}')\n",
    "        paragraph = 'Válka je špatná a inflace také.'\n",
    "        paragraph_encodings = tokenizer.encode(paragraph, return_tensors='pt')\n",
    "        pred = model.electra(paragraph_encodings)\n",
    "        # print(pred.last_hidden_state)\n",
    "        res = layer(pred.last_hidden_state)\n",
    "        fake_sentence_tokens = ['s'] + tokenizer.tokenize(paragraph) + ['k']\n",
    "        for token in fake_sentence_tokens:\n",
    "            print(\"{:>7s}\".format(token), end=\"\")\n",
    "        print()\n",
    "\n",
    "        for prediction in res.squeeze():\n",
    "            print(\"{:7.1f}\".format(prediction), end=\"\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = 'Válka je špatná a inflace také.'\n",
    "paragraph_encodings = tokenizer.encode(paragraph, return_tensors='pt')\n",
    "pred = model.electra(paragraph_encodings)\n",
    "# print(pred.last_hidden_state)\n",
    "res = layer(pred.last_hidden_state)\n",
    "fake_sentence_tokens = ['s'] + tokenizer.tokenize(paragraph) + ['k']\n",
    "for token in fake_sentence_tokens:\n",
    "    print(\"{:>7s}\".format(token), end=\"\")\n",
    "print()\n",
    "\n",
    "for prediction in res.squeeze():\n",
    "    print(\"{:7.1f}\".format(prediction), end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_filter import WordFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = WordFilter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5996633172035217\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.3    0.5    0.4    0.5    0.3    0.5    0.4    0.3    0.3\n",
      "0.5020788908004761\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.2    0.5    0.3    0.6    0.2    0.6    0.4    0.2    0.2\n",
      "0.42128250002861023\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.1    0.6    0.2    0.7    0.1    0.6    0.4    0.1    0.1\n",
      "0.3836243450641632\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    0.7    0.1    0.7    0.0    0.7    0.4    0.1    0.1\n",
      "0.3565623164176941\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    0.8    0.1    0.8    0.0    0.8    0.4    0.0    0.0\n",
      "0.39007654786109924\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    0.8    0.1    0.8    0.0    0.8    0.4    0.0    0.0\n",
      "0.3752056062221527\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    0.8    0.0    0.9    0.0    0.8    0.4    0.0    0.0\n",
      "0.2637668550014496\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    0.9    0.0    0.9    0.0    0.8    0.4    0.0    0.0\n",
      "0.2655162215232849\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    0.9    0.0    0.9    0.0    0.9    0.4    0.0    0.0\n",
      "0.20539849996566772\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    0.9    0.0    0.9    0.0    0.9    0.3    0.0    0.0\n",
      "0.2564178705215454\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    0.9    0.0    0.9    0.0    0.9    0.3    0.0    0.0\n",
      "0.26283618807792664\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    0.9    0.0    0.9    0.0    0.9    0.3    0.0    0.0\n",
      "0.28861454129219055\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    0.9    0.0    0.9    0.0    0.9    0.3    0.0    0.0\n",
      "0.3045003116130829\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    0.9    0.0    0.9    0.0    0.9    0.3    0.0    0.0\n",
      "0.33369699120521545\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    0.9    0.3    0.0    0.0\n",
      "0.16358859837055206\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    0.9    0.3    0.0    0.0\n",
      "0.33718541264533997\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    0.9    0.3    0.0    0.0\n",
      "0.19249385595321655\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    0.9    0.3    0.0    0.0\n",
      "0.19201356172561646\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    0.9    0.3    0.0    0.0\n",
      "0.13339434564113617\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    0.9    0.3    0.0    0.0\n",
      "0.1590566337108612\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.20131418108940125\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.1628149151802063\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.18955077230930328\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.24586690962314606\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.15486182272434235\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.21526236832141876\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.17786385118961334\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.19979070127010345\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.18538782000541687\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.208143949508667\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.22697360813617706\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.021359272301197052\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.22683845460414886\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.1792086809873581\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.18089504539966583\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.14423252642154694\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.10521200299263\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.2124512493610382\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.09255778789520264\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.16078943014144897\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.1    0.0    0.0\n",
      "0.1483207494020462\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.1    0.0    0.0\n",
      "0.2382734715938568\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.1    0.0    0.0\n",
      "0.22641147673130035\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.1    0.0    0.0\n",
      "0.21319152414798737\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.1    0.0    0.0\n",
      "0.13942879438400269\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.1    0.0    0.0\n",
      "0.11325222998857498\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.2564649283885956\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.13849462568759918\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.09671134501695633\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.1560453325510025\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.2    0.0    0.0\n",
      "0.16075195372104645\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.1    0.0    0.0\n",
      "0.40201592445373535\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.1    0.0    0.0\n",
      "0.13269762694835663\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.1    0.0    0.0\n",
      "0.12174659222364426\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.1    0.0    0.0\n",
      "0.1111140251159668\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.1    0.0    0.0\n",
      "0.18895824253559113\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.1    0.0    0.0\n",
      "0.1270047128200531\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.1    0.0    0.0\n",
      "0.08908148854970932\n",
      "      s  válka     je špatná      ainflace   také      .      k\n",
      "    0.0    1.0    0.0    1.0    0.0    1.0    0.1    0.0    0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [78], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m         labels_t\u001b[39m.\u001b[39mappend(labels[encoding\u001b[39m.\u001b[39mitem()])\n\u001b[0;32m     17\u001b[0m \u001b[39m# pred = model.electra(paragraph_encodings)\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mfilter\u001b[39m(paragraph_encodings, model\u001b[39m.\u001b[39melectra\u001b[39m.\u001b[39membeddings\u001b[39m.\u001b[39mword_embeddings)\n\u001b[0;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     22\u001b[0m loss_value \u001b[39m=\u001b[39m loss(res\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m), torch\u001b[39m.\u001b[39mtensor(labels_t))\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\school\\DAS\\parliament_spotlight\\Parliament_Spotlight_NLP\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\school\\DAS\\parliament_spotlight\\Parliament_Spotlight_NLP\\model\\word_filter.py:93\u001b[0m, in \u001b[0;36mWordFilter.forward\u001b[1;34m(self, x, embeddings)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, embeddings):\n\u001b[1;32m---> 93\u001b[0m     word_embeddings \u001b[39m=\u001b[39m embeddings(x)\n\u001b[0;32m     94\u001b[0m     \u001b[39m# batch = word_embeddings.shape[0]\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmain(word_embeddings\u001b[39m.\u001b[39mdetach())\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\school\\DAS\\parliament_spotlight\\Parliament_Spotlight_NLP\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\school\\DAS\\parliament_spotlight\\Parliament_Spotlight_NLP\\.env\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\stani\\Documents\\school\\DAS\\parliament_spotlight\\Parliament_Spotlight_NLP\\.env\\lib\\site-packages\\torch\\nn\\functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2193\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2194\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2196\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2197\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2199\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(filter.parameters(), lr=1e-4)\n",
    "\n",
    "labels = {word: 1.0 * (nulles[word] > 0.9) for word in nulles}\n",
    "counter = 0\n",
    "for paragraph in filter_builder.paragraphs():\n",
    "    counter += 1\n",
    "    labels_t = []\n",
    "    paragraph_encodings = tokenizer.encode(paragraph, return_tensors='pt')\n",
    "    if len(paragraph_encodings[0]) > 512:\n",
    "        paragraph_encodings = paragraph_encodings[:, 0:512]\n",
    "    for encoding in paragraph_encodings[0]:\n",
    "        if encoding.item() not in labels:\n",
    "            labels_t.append(0.0)\n",
    "        else:\n",
    "            labels_t.append(labels[encoding.item()])\n",
    "    # pred = model.electra(paragraph_encodings)\n",
    "    res = filter(paragraph_encodings, model.electra.embeddings.word_embeddings)\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_value = loss(res.squeeze(0).squeeze(1), torch.tensor(labels_t))\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    if counter % 1000 == 0:\n",
    "        print(f'{loss_value.item()}')\n",
    "        \n",
    "        paragraph = 'Válka je špatná a inflace také.'\n",
    "        paragraph_encodings = tokenizer.encode(paragraph, return_tensors='pt')\n",
    "        res = filter(paragraph_encodings, model.electra.embeddings.word_embeddings)\n",
    "        # print(res)\n",
    "        fake_sentence_tokens = ['s'] + tokenizer.tokenize(paragraph) + ['k']\n",
    "        for token in fake_sentence_tokens:\n",
    "            print(\"{:>7s}\".format(token), end=\"\")\n",
    "        print()\n",
    "\n",
    "        for prediction in res.squeeze():\n",
    "            print(\"{:7.1f}\".format(prediction), end=\"\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for word in nulles:\n",
    "    if nulles[word] > 0.99:\n",
    "        counter += 1\n",
    "print(counter / len(nulles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(filter.state_dict(), 'models\\\\filter.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      s děkuji      .  pěknédopoledne      , váženíkolegové      a    váž  ##ená   panípředsedkyně      .   jakopředseda    pod   ##vý ##boru    prodopravu   bych  chtěl krátce    oko ##ment ##ovat   tuto   norm    ##u      .    jak     už   bylo řečeno      ,   tato  norma   měla    býtprojedná   ##na     už      vminulémvolebním období      .   měla    být transp   ##on##ována     do      2      .  srpna     20   ##21      .     my   tady   mámezpoždění    rok      a    dva měsíce      a     ačevropská komiseprodlou ##žila   tuto  lhůtu     do      6      .      9      .     20   ##22      ,    tak    ani     to  nesti##hneme      a     už   bylozahájeno     to    san ##kční řízení   vůči  českérepublice      .      k\n",
      "    0.0    0.2    0.0    1.0    1.0    0.0    1.0    1.0    0.0    1.0    1.0    0.2    0.8    0.0    0.0    0.8    0.8    1.0    1.0    0.1    1.0    0.0    1.0    0.9    1.0    1.0    1.0    0.8    1.0    0.4    0.0    0.1    0.1    0.6    0.9    0.0    0.9    1.0    1.0    0.9    1.0    1.0    0.1    0.0    1.0    1.0    1.0    0.0    1.0    0.9    1.0    1.0    1.0    0.4    0.8    0.0    1.0    1.0    1.0    0.0    0.8    0.6    0.9    1.0    1.0    0.0    1.0    1.0    0.0    1.0    1.0    1.0    1.0    1.0    0.8    1.0    0.4    0.9    0.0    0.9    0.0    1.0    1.0    0.0    0.2    0.7    0.0    1.0    1.0    0.0    0.1    0.6    1.0    0.0    1.0    1.0    1.0    1.0    0.9    1.0    0.0    0.0\n"
     ]
    }
   ],
   "source": [
    "for paragraph in filter_builder.paragraphs():\n",
    "    \n",
    "    paragraph_encodings = tokenizer.encode(paragraph, return_tensors='pt')\n",
    "    res = filter(paragraph_encodings, model.electra.embeddings.word_embeddings)\n",
    "    # print(res)\n",
    "    fake_sentence_tokens = ['s'] + tokenizer.tokenize(paragraph) + ['k']\n",
    "    for token in fake_sentence_tokens:\n",
    "        print(\"{:>7s}\".format(token), end=\"\")\n",
    "    print()\n",
    "\n",
    "    for prediction in res.squeeze():\n",
    "        print(\"{:7.1f}\".format(prediction), end=\"\")\n",
    "    print()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "240a537d780be50fee6c75b11cefc8196c7bfe1ce873e8816879e02b4b181314"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
